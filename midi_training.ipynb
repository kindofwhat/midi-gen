{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MIDI Music Generation - Language Model Approach\n",
    "\n",
    "**Key insight**: Treat MIDI as a sequence of tokens (like text), not a piano roll.\n",
    "\n",
    "Using the proven tokenization from SkyTNT's midi-model:\n",
    "- Each MIDI event â†’ 8 tokens: `[event_type, time1, time2, track, channel, pitch, velocity, duration]`\n",
    "- Vocabulary: ~3400 tokens\n",
    "- Task: Predict next token (standard language modeling)\n",
    "\n",
    "This approach works because:\n",
    "1. Sparse representation (only events, not empty timesteps)\n",
    "2. Explicit structure (time, pitch, duration are separate tokens)\n",
    "3. Same approach as successful music models (MuseNet, Music Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:04.869695Z",
     "start_time": "2025-12-21T21:34:03.607311Z"
    }
   },
   "source": [
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from midi_tokenizer_wrapper import MIDITokenizerWrapper\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    print(\"Using Apple Silicon MPS\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:04.888616Z",
     "start_time": "2025-12-21T21:34:04.885314Z"
    }
   },
   "source": [
    "tokenizer = MIDITokenizerWrapper(version=\"v2\", optimise_midi=True)\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_id\n",
    "BOS_ID = tokenizer.bos_id\n",
    "EOS_ID = tokenizer.eos_id\n",
    "MAX_TOKEN_SEQ = tokenizer.max_token_seq  # 8 tokens per event\n",
    "\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Tokens per event: {MAX_TOKEN_SEQ}\")\n",
    "print(f\"Special tokens: PAD={PAD_ID}, BOS={BOS_ID}, EOS={EOS_ID}\")\n",
    "print(f\"\\nEvent types: {list(tokenizer.get_event_info()['events'].keys())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3406\n",
      "Tokens per event: 8\n",
      "Special tokens: PAD=0, BOS=1, EOS=2\n",
      "\n",
      "Event types: ['note', 'patch_change', 'control_change', 'set_tempo', 'time_signature', 'key_signature']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:05.528744Z",
     "start_time": "2025-12-21T21:34:04.900379Z"
    }
   },
   "source": [
    "DATA_DIR = Path(\"./midi_data\")\n",
    "ADL_DIR = DATA_DIR / \"adl-piano-midi\"\n",
    "\n",
    "# Get all MIDI files\n",
    "all_midi_files = list(ADL_DIR.rglob(\"*.mid\"))\n",
    "print(f\"Found {len(all_midi_files)} MIDI files\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(all_midi_files)\n",
    "n = len(all_midi_files)\n",
    "train_files = all_midi_files[:int(n * 0.8)]\n",
    "val_files = all_midi_files[int(n * 0.8):int(n * 0.9)]\n",
    "test_files = all_midi_files[int(n * 0.9):]\n",
    "\n",
    "print(f\"Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11076 MIDI files\n",
      "Train: 8860, Val: 1108, Test: 1108\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:13.368367Z",
     "start_time": "2025-12-21T21:34:05.568669Z"
    }
   },
   "source": [
    "# Limit for faster iteration (set to None for full dataset)\n",
    "MAX_FILES = 500  # Start small\n",
    "\n",
    "def tokenize_files(files, max_files=None):\n",
    "    \"\"\"Tokenize MIDI files and return flattened sequences.\"\"\"\n",
    "    if max_files:\n",
    "        files = files[:max_files]\n",
    "    \n",
    "    sequences = []\n",
    "    for f in tqdm(files, desc=\"Tokenizing\"):\n",
    "        tokens = tokenizer.midi_file_to_tokens(f)\n",
    "        if tokens is not None and len(tokens) > 10:  # Skip very short files\n",
    "            # Flatten: (num_events, 8) -> (num_events * 8,)\n",
    "            flat = tokenizer.flatten_tokens(tokens)\n",
    "            sequences.append(flat)\n",
    "    return sequences\n",
    "\n",
    "print(\"Tokenizing training data...\")\n",
    "train_sequences = tokenize_files(train_files, MAX_FILES)\n",
    "print(f\"Tokenized {len(train_sequences)} training files\")\n",
    "\n",
    "print(\"\\nTokenizing validation data...\")\n",
    "val_sequences = tokenize_files(val_files, MAX_FILES // 4)\n",
    "print(f\"Tokenized {len(val_sequences)} validation files\")\n",
    "\n",
    "# Stats\n",
    "total_tokens = sum(len(s) for s in train_sequences)\n",
    "avg_len = total_tokens / len(train_sequences) if train_sequences else 0\n",
    "print(f\"\\nTotal training tokens: {total_tokens:,}\")\n",
    "print(f\"Average sequence length: {avg_len:.0f} tokens\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing:   0%|          | 0/500 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c3731556b7c4c649c0b4792ec2381c1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 491 training files\n",
      "\n",
      "Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing:   0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8356598c215f4c0f81721db7b0f3014e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 124 validation files\n",
      "\n",
      "Total training tokens: 4,540,320\n",
      "Average sequence length: 9247 tokens\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:13.436237Z",
     "start_time": "2025-12-21T21:34:13.428610Z"
    }
   },
   "source": [
    "SEQUENCE_LENGTH = 512  # Context window (tokens)\n",
    "STRIDE = 256  # Overlap between sequences\n",
    "\n",
    "class TokenSequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for next-token prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, seq_length, stride):\n",
    "        self.samples = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Create sliding windows\n",
    "            for i in range(0, len(seq) - seq_length, stride):\n",
    "                self.samples.append(seq[i:i + seq_length + 1])  # +1 for target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        x = torch.LongTensor(sample[:-1])  # Input\n",
    "        y = torch.LongTensor(sample[1:])   # Target (shifted by 1)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = TokenSequenceDataset(train_sequences, SEQUENCE_LENGTH, STRIDE)\n",
    "val_dataset = TokenSequenceDataset(val_sequences, SEQUENCE_LENGTH, STRIDE)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "# Check a sample\n",
    "x, y = train_dataset[0]\n",
    "print(f\"\\nSample input shape: {x.shape}\")\n",
    "print(f\"Sample target shape: {y.shape}\")\n",
    "print(f\"First 16 tokens: {x[:16].tolist()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 17,000\n",
      "Val samples: 4,166\n",
      "\n",
      "Sample input shape: torch.Size([512])\n",
      "Sample target shape: torch.Size([512])\n",
      "First 16 tokens: [1, 0, 0, 0, 0, 0, 0, 0, 7, 9, 137, 2201, 3370, 3386, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:13.441987Z",
     "start_time": "2025-12-21T21:34:13.439544Z"
    }
   },
   "source": [
    "BATCH_SIZE = 32 if DEVICE.type in ('mps', 'cuda') else 16\n",
    "NUM_WORKERS = 0 if DEVICE.type == 'mps' else 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 532\n",
      "Val batches: 131\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Model Architecture\n",
    "\n",
    "We use a simple Transformer decoder (GPT-style) for language modeling.\n",
    "This is simpler and more proven than Mamba for this task size."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:13.524385Z",
     "start_time": "2025-12-21T21:34:13.451228Z"
    }
   },
   "source": [
    "class MusicGPT(nn.Module):\n",
    "    \"\"\"Simple GPT-style model for music generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=6, \n",
    "                 max_seq_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer decoder layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            'causal_mask',\n",
    "            torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Causal mask for autoregressive generation\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        \n",
    "        # Transformer (using decoder with memory=x for self-attention only)\n",
    "        x = self.transformer(x, x, tgt_mask=mask, memory_mask=mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "model = MusicGPT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    max_seq_len=SEQUENCE_LENGTH + 64,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 8,215,374\n",
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:14.198375Z",
     "start_time": "2025-12-21T21:34:13.533532Z"
    }
   },
   "source": [
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 3e-4\n",
    "GRAD_CLIP = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Loss: CrossEntropy (ignoring PAD)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30\n",
      "Learning rate: 0.0003\n",
      "Loss: CrossEntropy (ignoring PAD)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:34:14.218985Z",
     "start_time": "2025-12-21T21:34:14.215508Z"
    }
   },
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device, grad_clip):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Reshape for cross entropy: (batch * seq_len, vocab) vs (batch * seq_len)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "cell-16",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-21T21:34:14.263587Z"
    }
   },
   "source": [
    "# Training loop\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model_path = DATA_DIR / \"best_model_lm.pt\"\n",
    "\n",
    "print(f\"Training MusicGPT on {DEVICE}...\")\n",
    "print(f\"Saving best model to: {best_model_path}\\n\")\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE, GRAD_CLIP)\n",
    "    val_loss = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Calculate perplexity (more interpretable for LM)\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    val_ppl = np.exp(val_loss)\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'vocab_size': VOCAB_SIZE,\n",
    "            'sequence_length': SEQUENCE_LENGTH,\n",
    "        }, best_model_path)\n",
    "        marker = \" *\"\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        marker = \"\"\n",
    "    \n",
    "    # Log\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    elapsed = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train: {train_loss:.4f} (ppl {train_ppl:.1f}) | \"\n",
    "          f\"Val: {val_loss:.4f} (ppl {val_ppl:.1f}) | \"\n",
    "          f\"LR: {lr:.6f} | {elapsed:.1f}s{marker}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nDone! Total: {total_time/60:.1f} min\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f} (perplexity: {np.exp(best_val_loss):.1f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MusicGPT on mps...\n",
      "Saving best model to: midi_data/best_model_lm.pt\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Train')\n",
    "ax1.plot(val_losses, label='Validation')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot([np.exp(l) for l in train_losses], label='Train')\n",
    "ax2.plot([np.exp(l) for l in val_losses], label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_title('Perplexity (lower is better)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel saved to: {best_model_path}\")\n",
    "print(\"Use midi_lm_generation.ipynb to generate music!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
