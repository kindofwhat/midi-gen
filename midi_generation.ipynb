{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MIDI Music Generation - Generate with Language Model\n",
    "\n",
    "Generate music using the trained MusicGPT model and SkyTNT tokenizer.\n",
    "\n",
    "**Requirements:** Trained model at `midi_data/best_model_lm.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:50:38.103650Z",
     "start_time": "2025-12-21T21:50:36.368994Z"
    }
   },
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from midi_tokenizer_wrapper import MIDITokenizerWrapper\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "DATA_DIR = Path(\"./midi_data\")\n",
    "OUTPUT_DIR = DATA_DIR / \"generated\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:50:38.159478Z",
     "start_time": "2025-12-21T21:50:38.154621Z"
    }
   },
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = MIDITokenizerWrapper(version=\"v2\", optimise_midi=True)\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "PAD_ID = tokenizer.pad_id\n",
    "BOS_ID = tokenizer.bos_id\n",
    "EOS_ID = tokenizer.eos_id\n",
    "\n",
    "print(f\"Vocabulary: {VOCAB_SIZE} tokens\")\n",
    "print(f\"Special: PAD={PAD_ID}, BOS={BOS_ID}, EOS={EOS_ID}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 3406 tokens\n",
      "Special: PAD=0, BOS=1, EOS=2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:50:39.421183Z",
     "start_time": "2025-12-21T21:50:38.214380Z"
    }
   },
   "source": [
    "# Model definition (same as training)\n",
    "class MusicGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=6, \n",
    "                 max_seq_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            'causal_mask',\n",
    "            torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        x = self.dropout(x)\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        x = self.transformer(x, x, tgt_mask=mask, memory_mask=mask)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "# Load model\n",
    "MODEL_PATH = DATA_DIR / \"best_model_lm.pt\"\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "SEQUENCE_LENGTH = checkpoint.get('sequence_length', 512)\n",
    "\n",
    "model = MusicGPT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=256, n_heads=8, n_layers=6,\n",
    "    max_seq_len=SEQUENCE_LENGTH + 64,\n",
    "    dropout=0.0  # No dropout at inference\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Val loss: {checkpoint['val_loss']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 7\n",
      "Val loss: 1.0762\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:51:01.340925Z",
     "start_time": "2025-12-21T21:51:01.335531Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, seed_tokens=None, max_events=256, \n",
    "             temperature=1.0, top_k=50, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate MIDI tokens autoregressively.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        tokenizer: MIDITokenizerWrapper\n",
    "        seed_tokens: Optional starting tokens (1D array)\n",
    "        max_events: Max events to generate (~8 tokens per event)\n",
    "        temperature: Sampling temperature (lower = more deterministic)\n",
    "        top_k: Keep only top k tokens\n",
    "        top_p: Keep tokens with cumulative prob <= p\n",
    "    \n",
    "    Returns:\n",
    "        Token sequence (2D: events x tokens_per_event)\n",
    "    \"\"\"\n",
    "    max_tokens = max_events * tokenizer.max_token_seq\n",
    "    \n",
    "    # Start with BOS or seed\n",
    "    if seed_tokens is None:\n",
    "        # Start with just BOS token padded\n",
    "        tokens = [tokenizer.bos_id] + [tokenizer.pad_id] * (tokenizer.max_token_seq - 1)\n",
    "    else:\n",
    "        tokens = list(seed_tokens)\n",
    "    \n",
    "    generated = list(tokens)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        # Prepare input (last SEQUENCE_LENGTH tokens)\n",
    "        context = generated[-SEQUENCE_LENGTH:]\n",
    "        x = torch.LongTensor([context]).to(DEVICE)\n",
    "        \n",
    "        # Get logits for next token\n",
    "        logits = model(x)[0, -1] / temperature\n",
    "        \n",
    "        # Top-k filtering\n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        generated.append(next_token)\n",
    "        \n",
    "        # Stop at EOS\n",
    "        if next_token == tokenizer.eos_id:\n",
    "            break\n",
    "    \n",
    "    # Reshape to events\n",
    "    result = np.array(generated)\n",
    "    # Pad to multiple of max_token_seq\n",
    "    pad_len = (tokenizer.max_token_seq - len(result) % tokenizer.max_token_seq) % tokenizer.max_token_seq\n",
    "    if pad_len > 0:\n",
    "        result = np.pad(result, (0, pad_len), constant_values=tokenizer.pad_id)\n",
    "    \n",
    "    return result.reshape(-1, tokenizer.max_token_seq)\n",
    "\n",
    "print(\"Generation function ready!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation function ready!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Generate Music"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:54:34.586844Z",
     "start_time": "2025-12-21T21:51:01.354585Z"
    }
   },
   "source": [
    "# Generate from scratch\n",
    "print(\"Generating music from scratch...\")\n",
    "generated_tokens = generate(\n",
    "    model, tokenizer,\n",
    "    max_events=500,  # ~500 notes/events\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(generated_tokens)} events\")\n",
    "print(f\"First 10 events:\\n{generated_tokens[:10]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating music from scratch...\n",
      "Generated 501 events\n",
      "First 10 events:\n",
      "[[   1    0    0    0    0    0    0    0]\n",
      " [   7    9  137 2201 3372 3386 3404 3404]\n",
      " [   8    9  137 2201 3396 3404 3404 3404]\n",
      " [3404 3404  137 2201 3394 3404 3404  185]\n",
      " [   4    9  137 2202 2329 2601 2600  185]\n",
      " [   4    9  137 2203 2330 2601 2600  185]\n",
      " [   4    9  137 2203 2330 2601 2600  185]\n",
      " [   3    9  137 2203 2330 2388 2600  185]\n",
      " [   3   10  137 2202 2329 2405 2600  169]\n",
      " [   3   10  137 2202 2329 2405 2600  169]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:54:56.677758Z",
     "start_time": "2025-12-21T21:54:56.667683Z"
    }
   },
   "source": [
    "# Save to MIDI\n",
    "output_path = OUTPUT_DIR / \"generated_lm.mid\"\n",
    "success = tokenizer.tokens_to_midi_file(generated_tokens, output_path)\n",
    "\n",
    "if success:\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"Failed to save MIDI\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: midi_data/generated/generated_lm.mid\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "cell-11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:54:56.817877Z",
     "start_time": "2025-12-21T21:54:56.689805Z"
    }
   },
   "source": "# Play the generated MIDI\nimport pretty_midi\n\ndef _has_fluidsynth():\n    \"\"\"Check if fluidsynth is available.\"\"\"\n    try:\n        import fluidsynth\n        return True\n    except ImportError:\n        return False\n\ndef play_midi(midi_path, sample_rate=22050):\n    \"\"\"Play a MIDI file with fallback if fluidsynth not available.\"\"\"\n    midi = pretty_midi.PrettyMIDI(str(midi_path))\n\n    if _has_fluidsynth():\n        audio = midi.fluidsynth(fs=sample_rate)\n    else:\n        # Fallback to basic synthesis\n        audio = midi.synthesize(fs=sample_rate)\n\n    return Audio(audio, rate=sample_rate)\n\nif output_path.exists():\n    print(\"Playing generated music:\")\n    play_midi(output_path)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing generated music:\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Generate with MIDI Seed\n",
    "\n",
    "Use an existing MIDI file as a starting point for generation."
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:54:56.832291Z",
     "start_time": "2025-12-21T21:54:56.828286Z"
    }
   },
   "source": [
    "# Load a seed MIDI\n",
    "seed_midi_path = DATA_DIR / \"adl-piano-midi/Classical/Classical/Johann Sebastian Bach/Aria, BWV 988.mid\"\n",
    "\n",
    "if seed_midi_path.exists():\n",
    "    seed_tokens_2d = tokenizer.midi_file_to_tokens(seed_midi_path)\n",
    "    if seed_tokens_2d is not None:\n",
    "        # Use first ~50 events as seed\n",
    "        seed_events = 50\n",
    "        seed_2d = seed_tokens_2d[:seed_events]\n",
    "        seed_flat = tokenizer.flatten_tokens(seed_2d)\n",
    "        \n",
    "        print(f\"Seed: {len(seed_2d)} events, {len(seed_flat)} tokens\")\n",
    "        \n",
    "        # Generate continuation\n",
    "        generated_tokens = generate(\n",
    "            model, tokenizer,\n",
    "            seed_tokens=seed_flat,\n",
    "            max_events=400,\n",
    "            temperature=0.85,\n",
    "            top_k=40\n",
    "        )\n",
    "        \n",
    "        output_path = OUTPUT_DIR / \"generated_from_seed_lm.mid\"\n",
    "        tokenizer.tokens_to_midi_file(generated_tokens, output_path)\n",
    "        print(f\"Saved to: {output_path}\")\n",
    "        \n",
    "        play_midi(output_path)\n",
    "else:\n",
    "    print(f\"Seed file not found: {seed_midi_path}\")\n",
    "    print(\"Try a different file from midi_data/adl-piano-midi/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed file not found: midi_data/adl-piano-midi/Classical/Classical/Johann Sebastian Bach/Aria, BWV 988.mid\n",
      "Try a different file from midi_data/adl-piano-midi/\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Explore Different Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-21T21:54:56.841239Z"
    }
   },
   "source": [
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nGenerating with temperature {temp}...\")\n",
    "    tokens = generate(model, tokenizer, max_events=300, temperature=temp)\n",
    "    \n",
    "    output_path = OUTPUT_DIR / f\"generated_lm_temp_{temp}.mid\"\n",
    "    tokenizer.tokens_to_midi_file(tokens, output_path)\n",
    "    print(f\"  Saved: {output_path.name}\")\n",
    "\n",
    "print(f\"\\nAll files saved to: {OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating with temperature 0.5...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "**Temperature guide:**\n",
    "- `0.5` - Very coherent, conservative\n",
    "- `0.8-0.9` - Balanced (recommended)\n",
    "- `1.0` - More varied\n",
    "- `1.2+` - Experimental, may be chaotic\n",
    "\n",
    "**For better results:**\n",
    "- Train for more epochs\n",
    "- Use larger dataset\n",
    "- Try different seeds from different genres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
